{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing import text\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "import gc\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainHoldoutSplit(df, dfLabel, holdoutSize=0.20):\n",
    "    from sklearn import model_selection \n",
    "    dfHoldOut = None\n",
    "    dfHoldOutLabel = None\n",
    "\n",
    "    cv_pre = model_selection.StratifiedShuffleSplit(n_splits=1, test_size=holdoutSize, random_state=1)\n",
    "    for train_index, test_index in cv_pre.split(df, dfLabel):\n",
    "        y_train, y_test = dfLabel[train_index], dfLabel[test_index]\n",
    "        x_train, x_test = df.iloc[train_index], df.iloc[test_index]\n",
    "        df, dfLabel = x_train, y_train\n",
    "        dfHoldOut, dfHoldOutLabel = x_test, y_test\n",
    "\n",
    "    print(\"==================== Data Set ==================================\")\n",
    "    print(\"Holdout Set => \", dfHoldOut.shape)\n",
    "    print(\"Train Set => \", df.shape)\n",
    "    print(\"==================== Data Set ==================================\")\n",
    "    return df, dfLabel, dfHoldOut, dfHoldOutLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying sum 23.31420588493347\n",
      "Calculating class_weight 23.596460580825806\n",
      "IPO 23.62406086921692\n",
      "Calculating class_weight_aux 60.835963010787964\n",
      "IPO 64.09716629981995\n",
      "Weights calculated 106.35609650611877\n",
      "==================== Data Set ==================================\n",
      "Holdout Set =>  (270732, 48)\n",
      "Train Set =>  (1534142, 48)\n",
      "==================== Data Set ==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pandas/core/series.py:842: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self.loc[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Data Set ==================================\n",
      "Holdout Set =>  (276146, 48)\n",
      "Train Set =>  (1257996, 48)\n",
      "==================== Data Set ==================================\n",
      "==========Train Sample Distribution =============\n",
      "0.0    985215\n",
      "1.0     84356\n",
      "Name: target, dtype: int64\n",
      "==========Validation Sample Distribution ========\n",
      "0.0    216204\n",
      "1.0     18296\n",
      "Name: target, dtype: int64\n",
      "==========Holdout Sample Distribution ===========\n",
      "0.0    249082\n",
      "1.0     21650\n",
      "Name: target, dtype: int64\n",
      "=================================================\n",
      "TIME: 168.86105155944824\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "# df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv', nrows=10000)\n",
    "# dfTest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv', nrows=100)\n",
    "df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "dfTest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
    "# df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "\n",
    "IDENTITY_COLUMNS = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n",
    "]\n",
    "AUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n",
    "TARGET_COLUMN = 'target'\n",
    "\n",
    "\n",
    "for column in IDENTITY_COLUMNS:\n",
    "    df[column] = df[column].apply(lambda x: True if x >= 0.5 else False)\n",
    "\n",
    "for column in AUX_COLUMNS:\n",
    "    df[column] = df[column].apply(lambda x: 1 if x >= 0.5 else 0.0)\n",
    "    \n",
    "print(\"Applying sum\", time.time() - st)\n",
    "df['label_imp'] = df[IDENTITY_COLUMNS].sum(axis=1)\n",
    "\n",
    "print(\"Calculating class_weight\", time.time() - st)\n",
    "class_weight = np.absolute(df[TARGET_COLUMN].values - np.mean(df[TARGET_COLUMN].values)) + 0.0001\n",
    "df['class_weight'] = class_weight\n",
    "\n",
    "max_weight = class_weight.max()\n",
    "print(\"IPO\", time.time() - st)\n",
    "# df['class_weight'] = df['class_weight'] + df['label_imp']\n",
    "# df['class_weight'] = df['class_weight'] .apply(lambda x: max_weight if x > 1 else 1 - x)\n",
    "\n",
    "df['class_weight'] = df[['class_weight', 'label_imp']].apply(lambda x: max_weight \n",
    "                                        if x['label_imp'] >= 1 else x['class_weight'], axis=1)\n",
    "\n",
    "print(\"Calculating class_weight_aux\", time.time() - st)\n",
    "class_weight_aux = np.absolute(df[AUX_COLUMNS] - np.mean(df[AUX_COLUMNS], axis=0)) + 0.0001\n",
    "df['class_weight_aux'] = pd.Series(class_weight_aux.values.tolist())\n",
    "print(\"IPO\", time.time() - st)\n",
    "\n",
    "df['class_weight_aux'] = df[['class_weight_aux', 'label_imp']].apply(lambda x: [max_weight] * len(AUX_COLUMNS) if x['label_imp'] >= 1 else x['class_weight_aux'], axis=1)\n",
    "print(\"Weights calculated\", time.time() - st)\n",
    "# df = pd.read_csv(\"../input/fake-news-detection/data.csv\")\n",
    "# df = df.fillna('')\n",
    "# df['comment_text'] = df['Headline']\n",
    "# df['pred'] = df['Label']\n",
    "# dfTest = df\n",
    "# df['id'] = df.index\n",
    "\n",
    "def filter_initial_sentence(x):\n",
    "    return \" \".join([re.sub('[^a-z0-9 ]+', '', x) if re.sub('[^a-z0-9 ]+', '', x) else x for x in x.lower().split()])\n",
    "\n",
    "def tokenize_initial_sentence(x):\n",
    "    x = re.sub(\"[’‘'“\\\"”.\\n\\t]+\", '', x.lower())\n",
    "    pat = re.compile(r\"([—!#$%&()*+,-./:;<=>?@\\[\\\\\\]^_`{|}~\\t\\n…])\")\n",
    "    x = re.sub(\"[0-9]+\", '', x.lower())\n",
    "    return pat.sub(\" \\\\1 \", x)\n",
    "\n",
    "df['comment_text'] = df['comment_text'].apply(tokenize_initial_sentence)\n",
    "dfTest['comment_text'] = dfTest['comment_text'].apply(tokenize_initial_sentence)\n",
    "\n",
    "df, dfLabel, dfHoldOut, dfHoldOutLabel = getTrainHoldoutSplit(df, df['target'], holdoutSize=0.15)\n",
    "df, dfLabel, dfVal, dfValLabel = getTrainHoldoutSplit(df, df['target'], holdoutSize=0.18)\n",
    "# Check Positive vs Negative samples\n",
    "print(\"==========Train Sample Distribution =============\")\n",
    "print(dfLabel.value_counts())\n",
    "print(\"==========Validation Sample Distribution ========\")\n",
    "print(dfValLabel.value_counts())\n",
    "print(\"==========Holdout Sample Distribution ===========\")\n",
    "print(dfHoldOutLabel.value_counts())\n",
    "print(\"=================================================\")\n",
    "print(\"TIME:\", time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>label_imp</th>\n",
       "      <th>male</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>class_weight_aux</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1363214</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355512</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258103</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915324</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042694</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.920131</td>\n",
       "      <td>[0.9201309827722046, 0.9201309827722046, 0.920...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166938</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124276</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202776</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845305</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520603</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084873</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421957</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311903</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139279</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319110</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412443</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081051</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933707</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.920131</td>\n",
       "      <td>[0.9201309827722046, 0.9201309827722046, 0.920...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196972</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262041</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954311</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983360</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808165</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268908</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654540</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258600</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709003</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504624</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798609</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774427</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464540</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478949</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195184</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949484</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201666</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557109</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815735</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366871</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628773</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350556</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674140</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294901</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164626</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481153</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439318</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890120</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044617</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460361</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503853</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563738</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031362</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67656</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119968</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510080</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710725</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631933</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206651</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820934</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99945</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.080069</td>\n",
       "      <td>[0.0800690172277954, 0.00010720271886015312, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1157395 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                        ...                                                           class_weight_aux\n",
       "1363214     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "355512      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "1258103     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "915324      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "1042694     0.0                        ...                          [0.9201309827722046, 0.9201309827722046, 0.920...\n",
       "1166938     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "1124276     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "1202776     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "845305      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "520603      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "1084873     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "1421957     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "311903      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "1139279     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "1319110     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "412443      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "1081051     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "933707      0.0                        ...                          [0.9201309827722046, 0.9201309827722046, 0.920...\n",
       "1196972     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "262041      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "954311      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "983360      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "808165      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "268908      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "1654540     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "258600      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "1709003     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "1504624     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "1798609     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "1774427     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "...         ...                        ...                                                                        ...\n",
       "1464540     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "478949      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "1195184     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "949484      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "201666      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "557109      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "815735      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "366871      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "628773      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "350556      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "1674140     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "294901      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "2697        0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "1164626     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "481153      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "1439318     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "890120      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "1044617     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "1460361     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "503853      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "563738      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "1031362     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "67656       0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "1119968     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "510080      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "1710725     0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "631933      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "206651      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "820934      0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "99945       0.0                        ...                          [0.0800690172277954, 0.00010720271886015312, 0...\n",
       "\n",
       "[1157395 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[['target', 'label_imp', 'male', 'class_weight', 'class_weight_aux']][(df['target'] < 0.5) & (df['female'])]\n",
    "df[['target', 'label_imp', 'male', 'class_weight', 'class_weight_aux']][df['target'] < 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer Keras used for tokenization and unique  word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713150\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "max_features = 2000000\n",
    "# tokenizer.fit_on_texts(df['comment_text'].tolist())\n",
    "tokenizer = text.Tokenizer(num_words = max_features, filters=\"\", lower=True, split=\" \")\n",
    "tokenizer.fit_on_texts(df['comment_text'].tolist() + dfHoldOut['comment_text'].tolist() + dfTest['comment_text'].tolist())\n",
    "\n",
    "print(len(tokenizer.word_index))\n",
    "tokenize_dict = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Loadup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file ../input/vector-jigsaw/jigsaw.pkl\n",
      "Loaded\n",
      "792179\n",
      "lookup_index: 713150\n",
      "Total Words: 713150 Total Unknow Words: 2138 Total Found: 711012\n",
      "Embeddings shape: (713151, 150)\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py:1137: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "(713151, 150)\n",
      "['cannot', 'gonna', 'gotta', 'wanna', 'gimme', '\\xa0', 'lemme', '\\xa0the', '\\u2004\\u2004\\u2004usa', '\\u2009', '\\xa0and', '\\xa0million', '\\xa0a', '\\u2009\\u2009', '\\xa0http', '\\u200aand', '\\xa0if', '\\xa0that', '\\u2002', '\\u2028', 'of\\xa0the', '✨\\u2005\\u2005', 'repatriate»', 'for\\xa0the', '\\xa0\\xa0', 'tombeau»', '☭\\u2004', 'growth\\u2028', '\\xa0people', '«sir', '»«', '\\u2009trillion', '»i', '\\xa0of', '\\xa0\\xa0\\xa0\\xa0\\xa0', '\\xa0is', 'that\\xa0there', '»theres', '\\xa0billion', 'a\\xa0', '\\u2028\\u2028•', 'c\\u2009', 'president\\xa0john', '\\xa0its', '\\u2009°c', '«superhero', 'act\\xa0', '«it', 'all\\xa0', '\\xa0obama', '\\xa0i', 'of«', 'at\\xa0\\xa0per', '«rhetorical', '\\x85us', 'ontario«regulation', '\\u3000', '\\r\\rfights', '\\r\\ri', 'all\\xa0the', '\\xa0in', 'greenbook\\xa0cuts', 'affairs\\xa0https', 'the\\xa0termination', 'fix\\xa0americas', 'system\\xa0in', 'the\\xa0bureau', 'the\\xa0', '§\\xa0', '\\xa0thats', 'of\\xa0', 'principle\\u2028go', '\\u2028used', 'peopleproblems\\xa0are', 'can\\xa0broker', 'east\\xa0and\\xa0revolutionize', 'by\\xa0', '\\xa0we', 'to\\xa0the', '\\xa0not', '\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0there', '\\xa0including', '\\xa0this', 'vegas\\xa0although', '\\xa0percent', 'month\\u200a', '»you', '\\u2004', 'targets\\u2028•supporting', 'district\\u2028•virtual', 'kelly\\xa0', 'peter\\xa0micciche\\xa0', 'bishop\\xa0', 'mike\\xa0dunleavy\\xa0', 'giessel\\xa0', 'anna\\xa0', 'mackinnon\\xa0', 'than\\xa0', 'problems\\u3000read', 'trust\\xa0that']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_embeddings_partial(embedings_filepath, tokenize_dict, embedding_size=300):\n",
    "    def load_embeddings_vec(path):\n",
    "        def nop(it, *a, **k):\n",
    "            return it\n",
    "\n",
    "        def get_coefs(word, *arr):\n",
    "            return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "        tqdm = nop\n",
    "        with open(path) as f:\n",
    "            return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n",
    "\n",
    "    def load_embeddings_pkl(path):\n",
    "        with open(path,'rb') as f:\n",
    "            emb_arr = pickle.load(f)\n",
    "        return emb_arr\n",
    "    print(\"Loading file\", embedings_filepath)\n",
    "    if embedings_filepath.endswith(\"pkl\"):\n",
    "        embedding_index_ = load_embeddings_pkl(embedings_filepath)\n",
    "    else:\n",
    "        embedding_index_ = load_embeddings_vec(embedings_filepath)\n",
    "    print(\"Loaded\")\n",
    "\n",
    "    embedding_index = {}\n",
    "    for key, value in embedding_index_.items():\n",
    "        embedding_index[key.lower()] = value\n",
    "    del embedding_index_\n",
    "\n",
    "    print(len(embedding_index))\n",
    "    embeddings = np.zeros((len(tokenize_dict) + 1, embedding_size))\n",
    "    unknown_words = []\n",
    "\n",
    "    tokens = list(tokenize_dict.keys())\n",
    "    found = 0\n",
    "    lookup_index = OrderedDict()\n",
    "    for i in range(len(tokens)):\n",
    "        try:\n",
    "            word = tokens[i]\n",
    "            lookup_index[word] = i\n",
    "            embeddings[i] = embedding_index[word]\n",
    "            found += 1\n",
    "        except Exception as err:\n",
    "            unknown_words.append(word)\n",
    "\n",
    "    del embedding_index\n",
    "    print(\"lookup_index:\", len(list(lookup_index.keys())))\n",
    "    \n",
    "    print(\"Total Words:\", len(tokenize_dict), \"Total Unknow Words:\", len(unknown_words), \"Total Found:\", found)\n",
    "    print(\"Embeddings shape:\", embeddings.shape)\n",
    "\n",
    "    \"\"\" Dictionary which gives index of word in embeddings,\n",
    "    Eg. lookup_index['word'] will return integer value, and at that index location in embeddings we can find its vector \"\"\"\n",
    "    lookup_index_tf = tf.constant(list(lookup_index.keys()))\n",
    "    table = tf.contrib.lookup.index_table_from_tensor(mapping=lookup_index_tf, default_value=lookup_index[\"-\"])\n",
    "    gc.collect()\n",
    "    return embeddings, table, unknown_words\n",
    "\n",
    "embedding_filepath = \"../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl\"\n",
    "embedding_filepath = \"../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl\"\n",
    "embedding_filepath = \"../input/vector-jigsaw/jigsaw.pkl\"\n",
    "# embedding_filepath = \"../input/crawl300d2m/crawl-300d-2M.vec\"\n",
    "# embedding_filepath = \"../input/glove-840b-300d/glove.840B.300d.txt\"\n",
    "\n",
    "embeddings, table, unknown_words = load_embeddings_partial(embedding_filepath, tokenize_dict,\n",
    "                                                           embedding_size=150)\n",
    "embeddings_tf = tf.placeholder(tf.float32, embeddings.shape, name='embeddings_tf')\n",
    "\n",
    "print(embeddings.shape)\n",
    "print(unknown_words[0:100])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Word2Vec whole vector load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# def _load_initial_models(w2v_file):\n",
    "#     print(\"Loading W2V Model\")\n",
    "#     model = KeyedVectors.load_word2vec_format(w2v_file, binary=True)\n",
    "#     dimentions = model.vector_size\n",
    "#     print(\"Dimention:\", dimentions)\n",
    "    \n",
    "#     embeddings = np.zeros((len(model.wv.vocab), dimentions))\n",
    "\n",
    "#     lookup_index = OrderedDict()\n",
    "#     for i in range(len(model.wv.vocab)):\n",
    "#             embedding_vector = model.wv[model.wv.index2word[i]]\n",
    "#             lookup_index[model.wv.index2word[i]] = i\n",
    "#             if embedding_vector is not None:\n",
    "#                     embeddings[i] = embedding_vector\n",
    "\n",
    "#     \"\"\" Dictionary which gives index of word in embeddings,\n",
    "#     Eg. lookup_index['word'] will return integer value, and at that index location in embeddings we can find its vector \"\"\"\n",
    "#     lookup_index_tf = tf.constant(list(lookup_index.keys()))\n",
    "#     table = tf.contrib.lookup.index_table_from_tensor(mapping=lookup_index_tf, default_value=lookup_index['``'])\n",
    "#     return embeddings, table\n",
    "\n",
    "# w2v_file = \"../input/word2vec-google/GoogleNews-vectors-negative300.bin\"\n",
    "# w2v_file = \"../input/googlenewsvectorsnegative300slim/googlenews-vectors-negative300-slim.bin/GoogleNews-vectors-negative300-SLIM.bin\"\n",
    "# w2v_file = \"../input/jigsaw-w2v/jigsaw_model.bin\"\n",
    "# embeddings, table = _load_initial_models(w2v_file)\n",
    "# embeddings_tf = tf.placeholder(tf.float32, embeddings.shape, name='embeddings_tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x  = 150\n",
    "n_a = 108\n",
    "n_y = 2\n",
    "mod = 50\n",
    "n_y_aux = 6\n",
    "num_layers = 3\n",
    "num_memoryunit = 1\n",
    "MINORFLOAT  = 0.00000001\n",
    "MODEL_NAME = \"BI-LSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.framework.ops.Graph object at 0x7f34e4e4a1d0>\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-9-a5763332b73f>:29: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-9-a5763332b73f>:32: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/rnn.py:233: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "outputs: Tensor(\"stack_bidirectional_rnn/cell_2/concat:0\", shape=(?, ?, 216), dtype=float32)\n",
      "outputs: Tensor(\"concat:0\", shape=(?, 432), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-9-a5763332b73f>:85: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "Tensor(\"logits/LeakyRelu:0\", shape=(?, 2), dtype=float32)\n",
      "Tensor(\"logits_aux/LeakyRelu:0\", shape=(?, 12), dtype=float32)\n",
      "Tensor(\"loss_all:0\", shape=(?, ?), dtype=float32)\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f34e4e4a1d0>\n"
     ]
    }
   ],
   "source": [
    "# tf.reset_default_graph()\n",
    "results = {}\n",
    "G = tf.Graph()\n",
    "print(G)\n",
    "with G.as_default():\n",
    "    counter = tf.Variable(0, name='counter', trainable=False)\n",
    "    assert counter.graph is G\n",
    "    \n",
    "    xav_init = tf.contrib.layers.xavier_initializer\n",
    "    keep_prob = tf.placeholder(dtype=tf.float32, name='keep_prob')\n",
    "    learning_rate = tf.placeholder(dtype=tf.float32, name='learning_rate')\n",
    "    x = tf.placeholder(shape=[None, None, n_x], dtype=tf.float32, name='x')\n",
    "    y = tf.placeholder(shape=[None], dtype=tf.int32, name='y')\n",
    "    y_aux = tf.placeholder(shape=[None, n_y_aux], dtype=tf.int32, name='y_aux')\n",
    "    class_weights = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "    class_weights_aux = tf.placeholder(shape=[None, n_y_aux], dtype=tf.float32)\n",
    "\n",
    "    xt = x\n",
    "#     xt = tf.transpose(x, [1, 0, 2])\n",
    "    yt = tf.reshape(y, [-1])  # check this print yt and yb\n",
    "\n",
    "    \"\"\" Multiple LSTM CELLS \"\"\"\n",
    "    # Forward direction cell\n",
    "    lstm_fw_cells = []\n",
    "    for _ in range(num_layers):\n",
    "            lstm_fw_cells_m = []\n",
    "            for _ in range(num_memoryunit):\n",
    "                \n",
    "                    lstm_fw_cell_m = tf.nn.rnn_cell.GRUCell(n_a, kernel_initializer=xav_init())\n",
    "#                     lstm_fw_cell_m = tf.contrib.rnn.BasicLSTMCell(n_a, state_is_tuple=True)\n",
    "                    lstm_fw_cells_m.append(lstm_fw_cell_m)\n",
    "            lstm_fw_cells_m = tf.nn.rnn_cell.MultiRNNCell(lstm_fw_cells_m, state_is_tuple=True)\n",
    "            lstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_fw_cells_m, input_keep_prob=keep_prob)\n",
    "            lstm_fw_cells.append(lstm_fw_cell)\n",
    "\n",
    "    # Backward direction cell\n",
    "    lstm_bw_cells = []\n",
    "    for _ in range(num_layers):\n",
    "            lstm_bw_cells_m = []\n",
    "            for _ in range(num_memoryunit):\n",
    "                    lstm_bw_cell_m = tf.nn.rnn_cell.GRUCell(n_a, kernel_initializer=xav_init())\n",
    "#                     lstm_bw_cell_m = tf.contrib.rnn.BasicLSTMCell(n_a, state_is_tuple=True)\n",
    "                    lstm_bw_cells_m.append(lstm_bw_cell_m)\n",
    "            lstm_bw_cells_m = tf.nn.rnn_cell.MultiRNNCell(lstm_bw_cells_m, state_is_tuple=True)\n",
    "            lstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_bw_cells_m, input_keep_prob=keep_prob)\n",
    "            lstm_bw_cells.append(lstm_bw_cell)\n",
    "\n",
    "    \"\"\" This is stacking different layers \"\"\"\n",
    "    outputs_, _, _= tf.contrib.rnn.stack_bidirectional_dynamic_rnn(cells_fw=lstm_fw_cells, cells_bw=lstm_bw_cells, inputs=xt, dtype=tf.float32)\n",
    "    print(\"outputs:\", outputs_)\n",
    "    \n",
    "#     ''' Picking last activation '''\n",
    "#     outputs = outputs_[:, -1, :]\n",
    "#     print(\"outputs:\", outputs)\n",
    "    \n",
    "    ''' Max and Avg Pool Activation '''\n",
    "    mean = tf.math.reduce_mean(outputs_, axis=1)\n",
    "    maximun = tf.math.reduce_max(outputs_, axis=1)\n",
    "    outputs = tf.concat([mean, maximun], axis=1)\n",
    "    print(\"outputs:\", outputs)\n",
    "\n",
    "#     \"\"\" START \"\"\"\n",
    "#     outputs_, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "#         tf.contrib.rnn.BasicLSTMCell(n_a, state_is_tuple=True), tf.contrib.rnn.BasicLSTMCell(n_a, state_is_tuple=True),\n",
    "#         inputs=xt, dtype=tf.float32\n",
    "#     )\n",
    "#     print(\"outputs\", outputs_)\n",
    "#     outputs_ = tf.concat(outputs_, 2)\n",
    "#     outputs = outputs_[:, -1, :]\n",
    "\n",
    "    \"\"\" END \"\"\"\n",
    "#     #     \"\"\" Single LSTM CELL \"\"\"\n",
    "#     lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_a, state_is_tuple=True)\n",
    "#     outputs_, _ = tf.nn.dynamic_rnn(lstm_cell, xt, dtype=tf.float32)\n",
    "\n",
    "#     ''' CUDA LSTM Bidirectional short version '''\n",
    "#     lstm = tf.contrib.cudnn_rnn.CudnnLSTM(num_layers=num_layers, num_units=n_a, \n",
    "#                                           kernel_initializer = xav_init(),\n",
    "#                                           dropout= 0.0, direction=\"bidirectional\")\n",
    "#     outputs_, output_states = lstm(xt)\n",
    "#     outputs = outputs_[:, -1, :]\n",
    "\n",
    "    with tf.variable_scope(\"dnn1\") as scope:\n",
    "        dense = tf.layers.dense(inputs=outputs, units=n_a * 4, kernel_initializer=xav_init(),\n",
    "                                activation=tf.nn.leaky_relu)\n",
    "\n",
    "    with tf.variable_scope(\"dnn2\") as scope:\n",
    "        dense = tf.layers.dense(inputs=dense, units=n_a, kernel_initializer=xav_init(), \n",
    "                            activation=tf.nn.leaky_relu)\n",
    "\n",
    "\n",
    "    logits = tf.layers.dense(inputs=dense,kernel_initializer=xav_init(), \n",
    "                             activation=tf.nn.leaky_relu, units=2, name='logits')\n",
    "\n",
    "    logits_aux = tf.layers.dense(inputs=dense,kernel_initializer=xav_init(), activation=tf.nn.leaky_relu, units=n_y_aux*2, name='logits_aux')\n",
    "\n",
    "    print(logits)\n",
    "    print(logits_aux)\n",
    "\n",
    "    predictions_softmax = tf.nn.softmax(logits)\n",
    "    predictions_argmax  =  tf.argmax(predictions_softmax, axis=1)\n",
    "\n",
    "    train_vars   = tf.trainable_variables() \n",
    "    l2_loss = tf.add_n([ tf.nn.l2_loss(v) for v in train_vars if 'bias' not in v.name ]) * 0.001\n",
    "    l2_loss_mean = tf.reduce_mean(l2_loss)\n",
    "    \n",
    "#     yt_onehot = tf.one_hot(yt, n_y)\n",
    "    #     loss_all = tf.nn.weighted_cross_entropy_with_logits(logits=logits, targets=yt_onehot, pos_weight=classes_weights)\n",
    "#     weight_per_label = tf.linalg.matmul(tf.to_float(yt_onehot), tf.transpose(class_weights)) # shape [batch_size, 2]\n",
    "\n",
    "    loss_all = tf.math.multiply(class_weights, tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=yt), name='loss_all')\n",
    "    loss_all_mean = tf.reduce_mean(loss_all)\n",
    "\n",
    "    logits_auxs = [x for i in range(n_y_aux)]\n",
    "    loss_all_auxs = [x for i in range(n_y_aux)]\n",
    "    for i in range(n_y_aux):\n",
    "        logits_auxs[i] = logits_aux[:, 1:3]\n",
    "        y_aux_i = y_aux[:, i]\n",
    "        y_aux_i = tf.reshape(y_aux_i, [-1])\n",
    "        loss_all_auxs[i] = tf.math.multiply(class_weights_aux[:, i], tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_auxs[i], labels=y_aux_i), \n",
    "                                            name='loss_aux' + str(i))\n",
    "    # loss_train = loss_all + 0.5 * (1 - f1)    \n",
    "    print(loss_all)\n",
    "    loss_train = loss_all + l2_loss_mean * 0.005 + loss_all_auxs[0] + loss_all_auxs[1] + loss_all_auxs[2] + loss_all_auxs[3] + loss_all_auxs[4] + loss_all_auxs[5]\n",
    "    loss = tf.reduce_mean(loss_train)\n",
    "\n",
    "    tf.summary.scalar('loss_modified', loss)\n",
    "    tf.summary.scalar('loss', tf.reduce_mean(loss_all))\n",
    "\n",
    "    #     optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    #     tvars = tf.trainable_variables()\n",
    "    #     grads, _ = tf.clip_by_global_norm(tf.gradients(loss_all, tvars), 10)\n",
    "    #     train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_train)\n",
    "    global_step = tf.assign_add(counter, 1, name='global_step')\n",
    "\n",
    "    cls_sess = tf.Session()\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    cls_sess.run(init_op)\n",
    "    print(loss.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n",
    "\n",
    "def compute_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def compute_subgroup_auc(df, subgroup, label, model_name):\n",
    "    subgroup_examples = df[df[subgroup]]\n",
    "    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n",
    "\n",
    "def compute_bpsn_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n",
    "    subgroup_negative_examples = df[df[subgroup] & df[label] <= 0.5]\n",
    "    non_subgroup_positive_examples = df[~df[subgroup] & df[label] > 0.5]\n",
    "    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "def compute_bnsp_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n",
    "    subgroup_positive_examples = df[df[subgroup] & df[label] > 0.5]\n",
    "    non_subgroup_negative_examples = df[~df[subgroup] & df[label] <= 0.5]\n",
    "    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "def compute_bias_metrics_for_model(dataset,\n",
    "                                   subgroups,\n",
    "                                   model,\n",
    "                                   label_col,\n",
    "                                   include_asegs=False):\n",
    "    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n",
    "    records = []\n",
    "    for subgroup in subgroups:\n",
    "        record = {\n",
    "            'subgroup': subgroup,\n",
    "            'subgroup_size': len(dataset[dataset[subgroup]])\n",
    "        }\n",
    "        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n",
    "        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n",
    "        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n",
    "\n",
    "def calculate_overall_auc(df, model_name):\n",
    "    true_labels = df[TARGET_COLUMN]\n",
    "    predicted_labels = df[model_name]\n",
    "    return metrics.roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "def power_mean(series, p):\n",
    "    total = sum(np.power(series, p))\n",
    "    return np.power(total / len(series), 1 / p)\n",
    "\n",
    "def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n",
    "    print(\"Overall AUC:\", overall_auc)\n",
    "    bias_score = np.average([\n",
    "        power_mean(bias_df[SUBGROUP_AUC], POWER),\n",
    "        power_mean(bias_df[BPSN_AUC], POWER),\n",
    "        power_mean(bias_df[BNSP_AUC], POWER)\n",
    "    ])\n",
    "    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Iterations: 1228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2K #### Train Progress 100 Loss(avg): 0.22 P: 0.74 R: 0.46 F1: 0.57 Acc: 0.94 Time: 4.54"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AUC: 0.9327519672897158\n",
      "AUC_SCORE: 0.9041875554070362\n",
      "1708.8245990276337\n",
      "Iterations: 269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2K #### Validation Eval Progress  100 Loss(avg): 0.21 P: 0.68 R: 0.6 F1: 0.64 Acc: 0.95 Time: 1.48"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AUC: 0.9474375443098653\n",
      "AUC_SCORE: 0.9218979154894316\n",
      "233.28076171875\n",
      "Epoch: 1\n",
      "Iterations: 1228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2K #### Train Progress 100 Loss(avg): 0.21 P: 0.76 R: 0.51 F1: 0.61 Acc: 0.95 Time: 4.47"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AUC: 0.9475057982367\n",
      "AUC_SCORE: 0.9249757149974608\n",
      "1892.8943955898285\n",
      "Iterations: 269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2K #### Validation Eval Progress  100 Loss(avg): 0.21 P: 0.75 R: 0.54 F1: 0.63 Acc: 0.95 Time: 1.6"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AUC: 0.9511350733155777\n",
      "AUC_SCORE: 0.9299171861202664\n",
      "271.54914832115173\n",
      "Epoch: 2\n",
      "Iterations: 1228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2K #### Train Progress 100 Loss(avg): 0.21 P: 0.76 R: 0.51 F1: 0.61 Acc: 0.95 Time: 4.61"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AUC: 0.9494032883847348\n",
      "AUC_SCORE: 0.9277324751698244\n",
      "1901.0030169487\n",
      "Iterations: 269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2K #### Validation Eval Progress  100 Loss(avg): 0.21 P: 0.71 R: 0.58 F1: 0.64 Acc: 0.95 Time: 1.58"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AUC: 0.9509249540525702\n",
      "AUC_SCORE: 0.931262031804692\n",
      "264.9934277534485\n",
      "Iterations: 264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2K #### Holdout Eval Progress  100 Loss(avg): 0.21 P: 0.71 R: 0.59 F1: 0.64 Acc: 0.95 Time: 1.46"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall AUC: 0.951588443294957\n",
      "AUC_SCORE: 0.9317784582637656\n",
      "274.0971851348877\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "def parse_function(row):\n",
    "        row = table.lookup(row)\n",
    "        output = tf.nn.embedding_lookup(embeddings_tf, row)\n",
    "        \"\"\" See how can we add below features that were their in the original\n",
    "        pos_tag(sent)\n",
    "        getWordFeatures(token)\n",
    "        \"\"\"\n",
    "        return output\n",
    "\n",
    "def getDatasetIterator(ids, texts, labels, auxlabels, class_weight, class_weight_aux, mode=\"TRAIN\", batchsize=2, dimentions=100):\n",
    "    sentences = tf.data.Dataset.from_tensor_slices(texts)\n",
    "    sentences = sentences.map(lambda string: tf.string_split([string]).values)\n",
    "    #sentences = sentences.map(parse_function, num_parallel_calls=4)\n",
    "    sentences_embed = sentences.map(parse_function)\n",
    "    if mode in ['TRAIN', 'EVAL']:\n",
    "        ids =  tf.data.Dataset.from_tensor_slices(ids)\n",
    "        labels = tf.data.Dataset.from_tensor_slices(labels)        \n",
    "        auxlabels = tf.data.Dataset.from_tensor_slices(auxlabels)\n",
    "        class_weight = tf.data.Dataset.from_tensor_slices(class_weight)\n",
    "        class_weight_aux = tf.data.Dataset.from_tensor_slices(class_weight_aux.tolist())\n",
    "\n",
    "        dataset = tf.data.Dataset.zip((ids, sentences_embed, labels, auxlabels, class_weight, class_weight_aux))\n",
    "        if mode == \"TRAIN\":\n",
    "            dataset = dataset.shuffle(buffer_size=batchsize*2)\n",
    "\n",
    "        dataset = dataset.padded_batch(batchsize, padded_shapes=([], [None, dimentions],\n",
    "                                                                 [], [6], [], [6]))\n",
    "    elif mode in ['PREDICT']:\n",
    "        ids =  tf.data.Dataset.from_tensor_slices(ids)\n",
    "        dataset = tf.data.Dataset.zip((ids, sentences_embed))        \n",
    "        dataset = dataset.padded_batch(batchsize, padded_shapes=([], [None, dimentions]))\n",
    "    \n",
    "    dataset = dataset.prefetch(1)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    next_element = iterator.get_next()\n",
    "    init_op = iterator.initializer\n",
    "    return init_op, next_element\n",
    "\n",
    "def execute(df, batchsize=4, mode=\"TRAIN\", max_timestamps=200, comment=\"\"):\n",
    "    df['comment_text'] = df['comment_text'].apply(lambda x: x.replace(\"\\n\", ''))\n",
    "\n",
    "    init_op_model, next_element_model = getDatasetIterator(df['id'], \n",
    "                                            df['comment_text'],\n",
    "                                            df['target'] if mode in ['TRAIN', 'EVAL'] else None,\n",
    "                                            df[AUX_COLUMNS] if mode in ['TRAIN', 'EVAL'] else None,\n",
    "                                            df['class_weight'] if mode in ['TRAIN', 'EVAL'] else None,\n",
    "                                            df['class_weight_aux'] if mode in ['TRAIN', 'EVAL'] else None,\n",
    "                                            mode=mode, batchsize=batchsize, dimentions=n_x)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.tables_initializer())\n",
    "\n",
    "    print(\"Iterations:\", int(df.shape[0] / batchsize))\n",
    "    sess.run(init_op_model, feed_dict={embeddings_tf: embeddings})\n",
    "\n",
    "    eval_loss = []\n",
    "    train_loss = []\n",
    "    y_true_all = []\n",
    "    y_pred_all = []\n",
    "    predictions = []\n",
    "\n",
    "    while True:\n",
    "        st = time.time()\n",
    "        try:\n",
    "            batch = sess.run(next_element_model)\n",
    "            ids = batch[0]\n",
    "            xb = batch[1]\n",
    "            if mode in ['TRAIN', 'EVAL']:\n",
    "                yb = batch[2].astype(np.int)\n",
    "                yb_aux =  batch[3].astype(np.int)\n",
    "                class_weight = batch[4]\n",
    "                class_weight_aux = batch[5]\n",
    "\n",
    "            if xb.shape[1] > max_timestamps:\n",
    "                xb = xb[:, 0:max_timestamps, :]\n",
    "\n",
    "            if mode == \"TRAIN\":\n",
    "#                 class_weight = np.absolute(yb - np.mean(yb)) + 0.0001\n",
    "#                 class_weight_aux = np.absolute(yb_aux - np.mean(yb, axis=0)) + 0.0001\n",
    "                result = cls_sess.run([train_op, global_step, loss, predictions_argmax, yt, predictions_softmax],\n",
    "                                       feed_dict = {\n",
    "                                                x: xb, y: yb, y_aux: yb_aux, \n",
    "                                                keep_prob: 0.85, learning_rate: 0.005,\n",
    "                                                class_weights: class_weight.reshape(-1, 1),\n",
    "                                                class_weights_aux: class_weight_aux\n",
    "                                        })\n",
    "                global_step_, loss_, y_pred, y_true, y_softmax = result[1:6]\n",
    "                train_loss.append(loss_)\n",
    "                y_true_all.extend(y_true)\n",
    "                y_pred_all.extend(y_pred)\n",
    "                complete_percent = int((len(y_pred_all) / df.shape[0]) * 100 )\n",
    "                prediction = y_softmax\n",
    "                prediction = list(zip(ids, prediction[:, 1]))\n",
    "                predictions.extend(prediction)\n",
    "\n",
    "                if complete_percent < 3 or complete_percent > 99 or complete_percent % mod == 0 :\n",
    "                    cmd = [\"####\", comment, \"Progress\", str(complete_percent), \n",
    "                           \"Loss(avg):\", str(round(np.mean(train_loss), 2)), \n",
    "                           \"P:\", str(round(metrics.precision_score(y_true_all, y_pred_all), 2)),\n",
    "                          \"R:\", str(round(metrics.recall_score(y_true_all, y_pred_all), 2)),\n",
    "                          \"F1:\", str(round(metrics.f1_score(y_true_all, y_pred_all), 2)),\n",
    "                          \"Acc:\", str(round(metrics.accuracy_score(y_true_all, y_pred_all), 2)),\n",
    "                          'Time:', str(round(time.time() - st, 2))]\n",
    "                    cmd = \" \".join(cmd)\n",
    "                    sys.stderr.write('\\r\\033[2K %s' % (cmd))\n",
    "                    sys.stderr.flush()\n",
    "#                 results[global_step_] = {\n",
    "#                                         'P': round(metrics.precision_score(y_true, y_pred), 2),\n",
    "#                                         'R': round(metrics.recall_score(y_true, y_pred), 2),\n",
    "#                                         'F1': round(metrics.f1_score(y_true, y_pred), 2),\n",
    "#                                         'ACC': round(metrics.accuracy_score(y_true, y_pred), 2),\n",
    "#                                         'Loss': round(loss_, 3)\n",
    "#                 }\n",
    "            elif mode == \"EVAL\":\n",
    "#                 class_weight = np.absolute(yb - np.mean(yb)) + 0.0001\n",
    "#                 class_weight_aux = np.absolute(yb_aux - np.mean(yb, axis=0)) + 0.0001\n",
    "                result = cls_sess.run([loss, predictions_argmax, yt, predictions_softmax],\n",
    "                                       feed_dict = { \n",
    "                                                x: xb, y: yb, y_aux: yb_aux, keep_prob: 1.0,\n",
    "                                                class_weights: class_weight.reshape(-1, 1),\n",
    "                                                class_weights_aux: class_weight_aux\n",
    "                                       })\n",
    "\n",
    "                loss_, y_pred, y_true, y_softmax = result[0: 4]\n",
    "                eval_loss.append(loss_)\n",
    "                y_true_all.extend(y_true)\n",
    "                y_pred_all.extend(y_pred)\n",
    "                prediction = y_softmax\n",
    "                prediction = list(zip(ids, prediction[:, 1]))\n",
    "                predictions.extend(prediction)\n",
    "\n",
    "                complete_percent = int((len(y_pred_all) /  df.shape[0]) * 100)\n",
    "                if complete_percent < 3 or complete_percent > 99 or complete_percent % mod == 0 :\n",
    "                    cmd = [\"####\", comment, \"Progress \", str(complete_percent), \"Loss(avg):\", str(round(np.mean(eval_loss), 2)), \n",
    "                           \"P:\", str(round(metrics.precision_score(y_true_all, y_pred_all), 2)),\n",
    "                          \"R:\", str(round(metrics.recall_score(y_true_all, y_pred_all), 2)),\n",
    "                          \"F1:\", str(round(metrics.f1_score(y_true_all, y_pred_all), 2)),\n",
    "                          \"Acc:\", str(round(metrics.accuracy_score(y_true_all, y_pred_all), 2)),\n",
    "                          'Time:', str(round(time.time() - st, 2))]\n",
    "                    cmd = \" \".join(cmd)\n",
    "                    sys.stderr.write('\\r\\033[2K %s' % (cmd))\n",
    "                    sys.stderr.flush()\n",
    "            elif mode == \"PREDICT\":\n",
    "                result = cls_sess.run([predictions_softmax], feed_dict = { x: xb, keep_prob: 1.0 })\n",
    "                prediction = result[0]\n",
    "                prediction = list(zip(ids, prediction[:, 1]))\n",
    "                predictions.extend(prediction)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "\n",
    "    if mode in ['TRAIN', 'EVAL']:\n",
    "        resultdf = pd.DataFrame(predictions, columns=['id', MODEL_NAME])\n",
    "        df = pd.merge(df, resultdf, on='id', how='inner')\n",
    "        bias_metrics_df = compute_bias_metrics_for_model(df, IDENTITY_COLUMNS, MODEL_NAME, TARGET_COLUMN)\n",
    "        bias_metrics_df = bias_metrics_df.fillna(0.0)\n",
    "#         print(bias_metrics_df)    \n",
    "        auc_score = get_final_metric(bias_metrics_df, calculate_overall_auc(df, MODEL_NAME))\n",
    "        print(\"AUC_SCORE:\", auc_score)\n",
    "\n",
    "    gc.collect()\n",
    "    return predictions\n",
    "\n",
    "epoch = 3\n",
    "for i in range(epoch):\n",
    "    print(\"Epoch:\", i)\n",
    "    st = time.time()\n",
    "    execute(df, batchsize=1024, mode=\"TRAIN\", comment=\"Train\")\n",
    "    print(time.time() - st)\n",
    "    \n",
    "    st = time.time()\n",
    "    execute(dfVal, batchsize=1024, mode=\"EVAL\", comment=\"Validation Eval\")\n",
    "    print(time.time() - st)\n",
    "\n",
    "st = time.time()\n",
    "execute(dfHoldOut, batchsize=1024, mode=\"EVAL\", comment=\"Holdout Eval\")\n",
    "print(time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 95\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(97320, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = execute(dfTest, batchsize=1024, mode=\"PREDICT\", comment=\"Predictions\")\n",
    "dfResult = pd.DataFrame(predictions, columns=['id', 'prediction'])\n",
    "dfResult.to_csv(\"submission.csv\", index=False)\n",
    "dfResult.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# window = 5\n",
    "# iterations = sorted(results.keys())\n",
    "# f1 = pd.Series([results[x]['F1'] for x in iterations]).rolling(window=window).mean()\n",
    "# p = pd.Series([results[x]['P'] for x in iterations]).rolling(window=window).mean()\n",
    "# r = pd.Series([results[x]['R'] for x in iterations]).rolling(window=window).mean()\n",
    "# acc = pd.Series([results[x]['ACC'] for x in iterations]).rolling(window=window).mean()\n",
    "# loss_vals = pd.Series([results[x]['Loss'] for x in iterations]).rolling(window=window).mean()\n",
    "\n",
    "# rows = 3\n",
    "# cols = 2\n",
    "\n",
    "# def _plot(n, var, color,  xlabel, ylabel):\n",
    "#     plt.subplot(rows, cols, n)\n",
    "#     plt.plot(iterations, var, color=color)\n",
    "#     plt.xlabel(xlabel)\n",
    "#     plt.ylabel(ylabel)\n",
    "\n",
    "# _plot(1, f1, 'orange', 'Iterations', 'F1')\n",
    "# _plot(2, 'blue', p, 'Iterations', 'P')\n",
    "# _plot(3, 'black', r, 'Iterations', 'R')\n",
    "# _plot(4, 'green', acc, 'Iterations', 'Acc')\n",
    "# _plot(5, 'orange', loss_vals, 'Iterations', 'Loss')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
